"""
URL Resolver Utility
X·ª≠ l√Ω vi·ªác resolve URL redirect ƒë·ªÉ l·∫•y URL cu·ªëi c√πng t·ª´ short links
"""
import requests
import logging
import re
from typing import Optional, Dict, Any, List
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class URLResolver:
    def __init__(self):
        self.timeout = 30  # TƒÉng timeout l√™n 30 gi√¢y
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Referer': 'https://m.taobao.com/'
        }
        
        # C√°c domain c·∫ßn resolve (comprehensive list)
        self.short_domains = [
            # Taobao short links
            'e.tb.cn',
            'tb.cn', 
            's.tb.cn',
            'm.tb.cn',
            's.click.taobao.com',
            'uland.taobao.com',
            
            # Taobao mobile domains
            'm.taobao.com',
            'h5.m.taobao.com',
            
            # Tmall mobile domains
            'm.tmall.com',
            'h5.tmall.com',
            
            # 1688 domains
            'qr.1688.com',
            'm.1688.com',
            'h5.1688.com'
        ]
        
        # C√°c domain ƒë√≠ch h·ª£p l·ªá (comprehensive list)
        self.target_domains = [
            # Taobao desktop
            'item.taobao.com',
            
            # Tmall desktop
            'detail.tmall.com',
            
            # 1688 desktop
            'detail.1688.com',
            
            # Mobile domains (also valid targets)
            'm.taobao.com',
            'h5.m.taobao.com',
            'm.tmall.com',
            'h5.tmall.com',
            'm.1688.com',
            'h5.1688.com'
        ]
        
        # Regex patterns ƒë·ªÉ extract URL t·ª´ text (comprehensive patterns)
        self.url_extraction_patterns = [
            # Pattern cho short links (∆∞u ti√™n cao nh·∫•t)
            r'https?://(?:qr\.1688\.com|e\.tb\.cn|tb\.cn|s\.tb\.cn|m\.tb\.cn|s\.click\.taobao\.com|uland\.taobao\.com)/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*',
            
            # Pattern cho mobile domains
            r'https?://(?:m\.taobao\.com|h5\.m\.taobao\.com|m\.tmall\.com|h5\.tmall\.com|m\.1688\.com|h5\.1688\.com)/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*',
            
            # Pattern cho desktop product URLs
            r'https?://(?:detail\.tmall\.com|item\.taobao\.com|detail\.1688\.com)/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*',
            
            # Pattern ch√≠nh: URL v·ªõi domain v√† path, d·ª´ng ·ªü k√Ω t·ª± kh√¥ng h·ª£p l·ªá
            r'https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*)?',
            
            # Fallback pattern cho c√°c URL kh√°c
            r'https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/[^\s]*',
        ]
        
        # Compiled patterns ƒë·ªÉ tƒÉng performance
        self.compiled_url_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.url_extraction_patterns]
    
    def is_short_url(self, url: str) -> bool:
        """Ki·ªÉm tra c√≥ ph·∫£i short URL kh√¥ng"""
        try:
            parsed = urlparse(url)
            return any(domain in parsed.netloc.lower() for domain in self.short_domains)
        except:
            return False
    
    def is_valid_target_url(self, url: str) -> bool:
        """Ki·ªÉm tra URL ƒë√≠ch c√≥ h·ª£p l·ªá kh√¥ng"""
        try:
            parsed = urlparse(url)
            return any(domain in parsed.netloc.lower() for domain in self.target_domains)
        except:
            return False
    
    def extract_urls_from_text(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ URL t·ª´ text c√≥ ch·ªØ tr∆∞·ªõc"""
        urls = []
        
        for pattern in self.compiled_url_patterns:
            matches = pattern.findall(text)
            urls.extend(matches)
        
        # Lo·∫°i b·ªè duplicate v√† clean up
        unique_urls = list(set(urls))
        cleaned_urls = [self._clean_url(url) for url in unique_urls]
        
        return [url for url in cleaned_urls if url]
    
    def _clean_url(self, url: str) -> str:
        """Clean up URL (lo·∫°i b·ªè k√Ω t·ª± th·ª´a ·ªü cu·ªëi)"""
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá ·ªü cu·ªëi URL
        url = url.rstrip('.,;!?Ôºâ„Äë„Äç ')
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i URL character ·ªü cu·ªëi
        import re
        url = re.sub(r'[^a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]+$', '', url)
        
        # ƒê·∫£m b·∫£o URL k·∫øt th√∫c h·ª£p l·ªá
        if url.endswith('/'):
            url = url[:-1]
            
        return url
    
    def extract_best_url_from_text(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t URL t·ªët nh·∫•t t·ª´ text (∆∞u ti√™n URL c√≥ v·∫ª l√† s·∫£n ph·∫©m)"""
        urls = self.extract_urls_from_text(text)
        
        if not urls:
            return None
        
        # ∆Øu ti√™n URL c√≥ ch·ª©a t·ª´ kh√≥a s·∫£n ph·∫©m
        product_keywords = ['taobao', 'tmall', '1688', 'item', 'detail', 'offer']
        
        for url in urls:
            if any(keyword in url.lower() for keyword in product_keywords):
                return url
        
        # N·∫øu kh√¥ng c√≥ URL n√†o ch·ª©a t·ª´ kh√≥a, tr·∫£ v·ªÅ URL ƒë·∫ßu ti√™n
        return urls[0]
    
    def get_final_url(self, short_url: str) -> Dict[str, Any]:
        """
        Resolve short URL th√†nh final URL
        H·ªó tr·ª£ c·∫£ URL thu·∫ßn t√∫y v√† text c√≥ ch·ªØ tr∆∞·ªõc URL
        
        Args:
            short_url: URL c·∫ßn resolve ho·∫∑c text ch·ª©a URL
            
        Returns:
            Dict ch·ª©a th√¥ng tin resolve result
        """
        try:
            logger.info(f"üîç Resolving URL: {short_url}")
            
            # Ki·ªÉm tra xem input c√≥ ph·∫£i l√† text ch·ª©a URL kh√¥ng
            extracted_url = None
            if not short_url.startswith(('http://', 'https://')):
                # C√≥ th·ªÉ l√† text ch·ª©a URL, th·ª≠ extract
                extracted_url = self.extract_best_url_from_text(short_url)
                if extracted_url:
                    logger.info(f"üìù Extracted URL from text: {extracted_url}")
                    short_url = extracted_url
                else:
                    return {
                        'success': False,
                        'original_url': short_url,
                        'final_url': None,
                        'error': 'Kh√¥ng t√¨m th·∫•y URL h·ª£p l·ªá trong text',
                        'method': 'no_url_found'
                    }
            
            # N·∫øu ƒë√£ l√† URL h·ª£p l·ªá, return lu√¥n
            if self.is_valid_target_url(short_url):
                return {
                    'success': True,
                    'original_url': short_url,
                    'final_url': short_url,
                    'redirect_count': 0,
                    'method': 'no_redirect_needed',
                    'extracted_from_text': extracted_url is not None
                }
            
            # N·∫øu kh√¥ng ph·∫£i short URL, nh∆∞ng c≈©ng kh√¥ng ph·∫£i target URL
            if not self.is_short_url(short_url):
                return {
                    'success': False,
                    'original_url': short_url,
                    'final_url': None,
                    'error': 'URL kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£',
                    'method': 'unsupported_domain',
                    'extracted_from_text': extracted_url is not None
                }
            
            # Resolve short URL v·ªõi multiple strategies
            final_url, redirect_count = self._resolve_with_strategies(short_url)
            
            logger.info(f"‚úÖ Resolved: {short_url} ‚Üí {final_url} ({redirect_count} redirects)")
            
            # Ki·ªÉm tra n·∫øu final_url l√† deep link, convert th√†nh web URL tr∆∞·ªõc
            if final_url.startswith(('wireless1688://', 'taobao://', 'tmall://')):
                logger.info(f"üîÑ Detected deep link, attempting conversion: {final_url}")
                # Extract content sau protocol
                deep_link_content = final_url.split('://', 1)[1] if '://' in final_url else final_url
                converted_url = self._convert_deep_link_to_web_url(deep_link_content, short_url)
                
                if converted_url and self.is_valid_target_url(converted_url):
                    logger.info(f"‚úÖ Successfully converted deep link: {final_url} ‚Üí {converted_url}")
                    return {
                        'success': True,
                        'original_url': short_url,
                        'final_url': converted_url,
                        'redirect_count': redirect_count + 1,
                        'method': 'deep_link_converted',
                        'extracted_from_text': extracted_url is not None
                    }
                else:
                    logger.warning(f"‚ùå Deep link conversion failed or invalid result: {converted_url}")
            
            # Ki·ªÉm tra n·∫øu final_url l√† mobile URL, convert th√†nh desktop URL
            if self._is_mobile_url(final_url):
                logger.info(f"üîÑ Detected mobile URL, attempting desktop conversion: {final_url}")
                desktop_url = self._convert_mobile_to_desktop_url(final_url)
                
                if desktop_url and self.is_valid_target_url(desktop_url):
                    logger.info(f"‚úÖ Successfully converted mobile to desktop: {final_url} ‚Üí {desktop_url}")
                    return {
                        'success': True,
                        'original_url': short_url,
                        'final_url': desktop_url,
                        'redirect_count': redirect_count + 1,
                        'method': 'mobile_to_desktop_converted',
                        'extracted_from_text': extracted_url is not None
                    }
                else:
                    logger.warning(f"‚ùå Mobile to desktop conversion failed: {desktop_url}")
            
            # Ki·ªÉm tra final URL c√≥ h·ª£p l·ªá kh√¥ng
            if not self.is_valid_target_url(final_url):
                # N·∫øu kh√¥ng c√≥ redirect, c√≥ th·ªÉ link ƒë√£ h·∫øt h·∫°n
                if redirect_count == 0:
                    error_msg = 'Link r√∫t g·ªçn c√≥ th·ªÉ ƒë√£ h·∫øt h·∫°n ho·∫∑c kh√¥ng ho·∫°t ƒë·ªông. Vui l√≤ng th·ª≠ link ƒë·∫ßy ƒë·ªß t·ª´ trang s·∫£n ph·∫©m.'
                else:
                    error_msg = f'Link redirect ƒë·∫øn trang kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {final_url}'
                
                return {
                    'success': False,
                    'original_url': short_url,
                    'final_url': final_url,
                    'redirect_count': redirect_count,
                    'error': error_msg,
                    'method': 'invalid_final_url'
                }
            
            return {
                'success': True,
                'original_url': short_url,
                'final_url': final_url,
                'redirect_count': redirect_count,
                'method': 'redirect_resolved',
                'extracted_from_text': extracted_url is not None
            }
            
        except requests.Timeout:
            logger.error(f"‚ùå Timeout khi resolve URL: {short_url}")
            return {
                'success': False,
                'original_url': short_url,
                'final_url': None,
                'error': 'Timeout khi resolve URL',
                'method': 'timeout'
            }
            
        except requests.RequestException as e:
            logger.error(f"‚ùå L·ªói request khi resolve URL {short_url}: {e}")
            return {
                'success': False,
                'original_url': short_url,
                'final_url': None,
                'error': f'L·ªói request: {str(e)}',
                'method': 'request_error'
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi resolve URL {short_url}: {e}")
            return {
                'success': False,
                'original_url': short_url,
                'final_url': None,
                'error': f'L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}',
                'method': 'unknown_error'
            }
    
    def _resolve_with_strategies(self, short_url: str) -> tuple:
        """
        Resolve URL: Th·ª≠ HTTP redirect tr∆∞·ªõc, n·∫øu kh√¥ng c√≥ th√¨ parse content
        Returns: (final_url, redirect_count)
        """
        try:
            logger.info(f"üîç Resolving: {short_url}")
            
            # Strategy 1: Th·ª≠ HTTP redirect tr∆∞·ªõc (HEAD request)
            try:
                response = requests.head(
                    short_url, 
                    headers=self.headers, 
                    allow_redirects=True, 
                    timeout=self.timeout
                )
            except requests.RequestException:
                # N·∫øu HEAD request th·∫•t b·∫°i, th·ª≠ GET request
                logger.info("HEAD request failed, trying GET request...")
                response = requests.get(
                    short_url, 
                    headers=self.headers, 
                    allow_redirects=True, 
                    timeout=self.timeout
                )
            
            final_url = response.url
            redirect_count = len(response.history)
            
            # N·∫øu c√≥ redirect, return ngay
            if redirect_count > 0 or self.is_valid_target_url(final_url):
                logger.info(f"‚úÖ HTTP redirect: {short_url} ‚Üí {final_url} ({redirect_count} redirects)")
                return final_url, redirect_count
            
            # Strategy 2: Th·ª≠ parse URL tr·ª±c ti·∫øp t·ª´ short URL
            logger.info("No HTTP redirect, trying direct URL parsing...")
            direct_result = self._parse_short_url_directly(short_url)
            if direct_result[1] > 0:  # N·∫øu c√≥ k·∫øt qu·∫£
                return direct_result
            
            # Strategy 3: N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£, parse content
            logger.info("No direct parsing result, trying content parsing...")
            content_result = self._parse_content_for_url(short_url)
            if content_result[1] > 0:  # N·∫øu c√≥ k·∫øt qu·∫£
                return content_result
            
            # Strategy 4: Th·ª≠ v·ªõi User-Agent kh√°c (desktop)
            logger.info("Content parsing failed, trying with desktop User-Agent...")
            return self._parse_with_desktop_ua(short_url)
            
        except requests.RequestException as e:
            logger.error(f"‚ùå Request failed: {e}")
            return short_url, 0
    
    def _parse_short_url_directly(self, short_url: str) -> tuple:
        """
        Parse short URL tr·ª±c ti·∫øp ƒë·ªÉ t√¨m product ID
        Returns: (final_url, redirect_count)
        """
        try:
            import re
            from urllib.parse import urlparse, parse_qs
            
            # Parse URL ƒë·ªÉ l·∫•y path v√† query parameters
            parsed = urlparse(short_url)
            path = parsed.path
            query_params = parse_qs(parsed.query)
            
            # T√¨m product ID trong path ho·∫∑c query parameters
            product_id = None
            
            # Th·ª≠ l·∫•y t·ª´ query parameters
            for param_name in ['id', 'itemId', 'item_id', 'productId']:
                if param_name in query_params:
                    product_id = query_params[param_name][0]
                    break
            
            # Th·ª≠ t√¨m ID trong path
            if not product_id:
                id_match = re.search(r'/(\d{9,13})', path)
                if id_match:
                    product_id = id_match.group(1)
            
            # Th·ª≠ t√¨m ID trong to√†n b·ªô URL
            if not product_id:
                id_match = re.search(r'(\d{9,13})', short_url)
                if id_match:
                    product_id = id_match.group(1)
            
            if product_id:
                # X√°c ƒë·ªãnh domain d·ª±a tr√™n short_url (comprehensive logic)
                if 'qr.1688.com' in short_url or '1688.com' in short_url:
                    final_url = f"https://detail.1688.com/offer/{product_id}.html"
                elif 'm.tb.cn' in short_url or 'm.taobao.com' in short_url or 'h5.m.taobao.com' in short_url:
                    final_url = f"https://item.taobao.com/item.htm?id={product_id}"
                elif 'm.tmall.com' in short_url or 'h5.tmall.com' in short_url or 'tmall' in short_url.lower():
                    final_url = f"https://detail.tmall.com/item.htm?id={product_id}"
                elif 'm.1688.com' in short_url or 'h5.1688.com' in short_url:
                    final_url = f"https://detail.1688.com/offer/{product_id}.html"
                else:
                    # Default to Taobao for unknown domains
                    final_url = f"https://item.taobao.com/item.htm?id={product_id}"
                
                logger.info(f"‚úÖ Direct parsing: {short_url} ‚Üí {final_url}")
                return final_url, 1
            
            return short_url, 0
            
        except Exception as e:
            logger.error(f"‚ùå Direct parsing failed: {e}")
            return short_url, 0
    
    def _parse_with_desktop_ua(self, short_url: str) -> tuple:
        """
        Parse URL v·ªõi desktop User-Agent
        Returns: (final_url, redirect_count)
        """
        try:
            # Desktop User-Agent
            desktop_headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Referer': 'https://www.taobao.com/'
            }
            
            # Th·ª≠ GET request v·ªõi desktop UA
            response = requests.get(
                short_url, 
                headers=desktop_headers, 
                allow_redirects=True,
                timeout=self.timeout
            )
            
            final_url = response.url
            redirect_count = len(response.history)
            
            # N·∫øu c√≥ redirect v√† URL h·ª£p l·ªá
            if redirect_count > 0 and self.is_valid_target_url(final_url):
                logger.info(f"‚úÖ Desktop UA redirect: {short_url} ‚Üí {final_url} ({redirect_count} redirects)")
                return final_url, redirect_count
            
            # N·∫øu kh√¥ng c√≥ redirect, th·ª≠ parse content
            content = response.text
            logger.info(f"Desktop UA got content: {len(content)} bytes")
            
            # T√¨m product ID trong content
            import re
            id_patterns = [
                r'itemId["\']?\s*:\s*["\']?(\d+)["\']?',
                r'item_id["\']?\s*:\s*["\']?(\d+)["\']?',
                r'id["\']?\s*:\s*["\']?(\d{9,13})["\']?',
                r'productId["\']?\s*:\s*["\']?(\d+)["\']?',
                r'[?&]id=(\d+)',
                r'[?&]itemId=(\d+)',
                r'[?&]item_id=(\d+)',
                r'[?&]productId=(\d+)'
            ]
            
            for pattern in id_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    product_id = matches[0]
                    
                    # X√°c ƒë·ªãnh domain d·ª±a tr√™n short_url
                    if 'm.tb.cn' in short_url or 'm.taobao.com' in short_url or 'h5.m.taobao.com' in short_url:
                        final_url = f"https://item.taobao.com/item.htm?id={product_id}"
                        logger.info(f"‚úÖ Desktop UA ID extraction: {short_url} ‚Üí {final_url}")
                        return final_url, 1
                    elif 'm.tmall.com' in short_url or 'h5.tmall.com' in short_url or 'tmall' in short_url.lower():
                        final_url = f"https://detail.tmall.com/item.htm?id={product_id}"
                        logger.info(f"‚úÖ Desktop UA Tmall ID extraction: {short_url} ‚Üí {final_url}")
                        return final_url, 1
                    elif 'qr.1688.com' in short_url or '1688.com' in short_url:
                        final_url = f"https://detail.1688.com/offer/{product_id}.html"
                        logger.info(f"‚úÖ Desktop UA 1688 ID extraction: {short_url} ‚Üí {final_url}")
                        return final_url, 1
            
            logger.warning("Desktop UA: No product ID found in content")
            return short_url, 0
            
        except Exception as e:
            logger.error(f"‚ùå Desktop UA parsing failed: {e}")
            return short_url, 0
    
    def _parse_content_for_url(self, short_url: str) -> tuple:
        """
        Parse HTML content ƒë·ªÉ t√¨m URL ƒë√≠ch
        Returns: (final_url, redirect_count)
        """
        try:
            # GET request ƒë·ªÉ l·∫•y content v·ªõi retry logic
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = requests.get(
                        short_url, 
                        headers=self.headers, 
                        timeout=self.timeout
                    )
                    break
                except requests.Timeout:
                    if attempt < max_retries - 1:
                        logger.warning(f"Timeout attempt {attempt + 1}, retrying...")
                        continue
                    else:
                        logger.error(f"All {max_retries} attempts timed out")
                        return short_url, 0
                except requests.RequestException as e:
                    logger.error(f"Request failed: {e}")
                    return short_url, 0
            
            content = response.text
            logger.info(f"Got content: {len(content)} bytes")
            
            # T√¨m product URLs trong content (comprehensive patterns)
            product_patterns = [
                # Desktop URLs
                r'href=["\']([^"\']*detail\.tmall\.com[^"\']*)["\']',
                r'href=["\']([^"\']*item\.taobao\.com[^"\']*)["\']',
                r'href=["\']([^"\']*detail\.1688\.com[^"\']*)["\']',
                r'url=["\']([^"\']*detail\.tmall\.com[^"\']*)["\']',
                r'url=["\']([^"\']*item\.taobao\.com[^"\']*)["\']',
                r'url=["\']([^"\']*detail\.1688\.com[^"\']*)["\']',
                r'["\']([^"\']*detail\.tmall\.com[^"\']*)["\']',
                r'["\']([^"\']*item\.taobao\.com[^"\']*)["\']',
                r'["\']([^"\']*detail\.1688\.com[^"\']*)["\']',
                
                # Mobile URLs
                r'href=["\']([^"\']*m\.taobao\.com[^"\']*)["\']',
                r'href=["\']([^"\']*h5\.m\.taobao\.com[^"\']*)["\']',
                r'href=["\']([^"\']*m\.tmall\.com[^"\']*)["\']',
                r'href=["\']([^"\']*h5\.tmall\.com[^"\']*)["\']',
                r'href=["\']([^"\']*m\.1688\.com[^"\']*)["\']',
                r'href=["\']([^"\']*h5\.1688\.com[^"\']*)["\']',
                
                r'url=["\']([^"\']*m\.taobao\.com[^"\']*)["\']',
                r'url=["\']([^"\']*h5\.m\.taobao\.com[^"\']*)["\']',
                r'url=["\']([^"\']*m\.tmall\.com[^"\']*)["\']',
                r'url=["\']([^"\']*h5\.tmall\.com[^"\']*)["\']',
                r'url=["\']([^"\']*m\.1688\.com[^"\']*)["\']',
                r'url=["\']([^"\']*h5\.1688\.com[^"\']*)["\']',
                
                r'["\']([^"\']*m\.taobao\.com[^"\']*)["\']',
                r'["\']([^"\']*h5\.m\.taobao\.com[^"\']*)["\']',
                r'["\']([^"\']*m\.tmall\.com[^"\']*)["\']',
                r'["\']([^"\']*h5\.tmall\.com[^"\']*)["\']',
                r'["\']([^"\']*m\.1688\.com[^"\']*)["\']',
                r'["\']([^"\']*h5\.1688\.com[^"\']*)["\']',
                
                # Deep link patterns (wireless1688://, taobao://, tmall://)
                r'wireless1688://([^"\']*)["\']?',
                r'taobao://([^"\']*)["\']?',
                r'tmall://([^"\']*)["\']?'
            ]
            
            import re
            for pattern in product_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    # X·ª≠ l√Ω deep links tr∆∞·ªõc
                    if 'wireless1688://' in pattern or 'taobao://' in pattern or 'tmall://' in pattern:
                        for match in matches:
                            # Convert deep link th√†nh web URL
                            web_url = self._convert_deep_link_to_web_url(match, short_url)
                            if web_url:
                                logger.info(f"‚úÖ Deep link conversion: {short_url} ‚Üí {web_url}")
                                return web_url, 1
                    
                    # Filter valid URLs (c√≥ id= v√† ƒë·ªß d√†i)
                    valid_matches = [m for m in matches if 'id=' in m and len(m) > 20]
                    if valid_matches:
                        final_url = valid_matches[0]
                        logger.info(f"‚úÖ Content parsing: {short_url} ‚Üí {final_url}")
                        return final_url, 1  # Simulate 1 redirect
            
            # N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m product ID
            id_patterns = [
                r'itemId["\']?\s*:\s*["\']?(\d+)["\']?',
                r'item_id["\']?\s*:\s*["\']?(\d+)["\']?',
                r'id["\']?\s*:\s*["\']?(\d{9,13})["\']?',
                r'productId["\']?\s*:\s*["\']?(\d+)["\']?',
                r'offerId=(\d+)',  # Th√™m pattern cho 1688 offerId
                r'offer\.id=(\d+)',  # Th√™m pattern kh√°c cho 1688
                r'offer/(\d+)\.html',  # Th√™m pattern t·ª´ URL path
                # Th√™m patterns cho mobile taobao
                r'itemId["\']?\s*=\s*["\']?(\d+)["\']?',
                r'item_id["\']?\s*=\s*["\']?(\d+)["\']?',
                r'id["\']?\s*=\s*["\']?(\d{9,13})["\']?',
                r'productId["\']?\s*=\s*["\']?(\d+)["\']?',
                # Th√™m patterns cho URL parameters
                r'[?&]id=(\d+)',
                r'[?&]itemId=(\d+)',
                r'[?&]item_id=(\d+)',
                r'[?&]productId=(\d+)'
            ]
            
            for pattern in id_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    product_id = matches[0]
                    
                    # X√°c ƒë·ªãnh domain d·ª±a tr√™n short_url (comprehensive logic)
                    if 'qr.1688.com' in short_url or '1688.com' in short_url:
                        # Construct 1688 URL
                        final_url = f"https://detail.1688.com/offer/{product_id}.html"
                        logger.info(f"‚úÖ 1688 ID extraction: {short_url} ‚Üí {final_url}")
                    elif 'm.tb.cn' in short_url or 'm.taobao.com' in short_url or 'h5.m.taobao.com' in short_url:
                        # Construct mobile Taobao URL
                        final_url = f"https://item.taobao.com/item.htm?id={product_id}"
                        logger.info(f"‚úÖ Mobile Taobao ID extraction: {short_url} ‚Üí {final_url}")
                    elif 'm.tmall.com' in short_url or 'h5.tmall.com' in short_url or 'tmall' in short_url.lower():
                        # Construct Tmall URL
                        final_url = f"https://detail.tmall.com/item.htm?id={product_id}"
                        logger.info(f"‚úÖ Tmall ID extraction: {short_url} ‚Üí {final_url}")
                    elif 'm.1688.com' in short_url or 'h5.1688.com' in short_url:
                        # Construct 1688 URL
                        final_url = f"https://detail.1688.com/offer/{product_id}.html"
                        logger.info(f"‚úÖ Mobile 1688 ID extraction: {short_url} ‚Üí {final_url}")
                    else:
                        # Construct URL (default to Taobao)
                        final_url = f"https://item.taobao.com/item.htm?id={product_id}"
                        logger.info(f"‚úÖ ID extraction: {short_url} ‚Üí {final_url}")
                    
                    return final_url, 1
            
            logger.warning("No product URL or ID found in content")
            return short_url, 0
            
        except Exception as e:
            logger.error(f"‚ùå Content parsing failed: {e}")
            return short_url, 0
    
    def _convert_deep_link_to_web_url(self, deep_link_content: str, original_short_url: str) -> Optional[str]:
        """
        Convert deep link content th√†nh web URL h·ª£p l·ªá
        Args:
            deep_link_content: N·ªôi dung c·ªßa deep link (kh√¥ng bao g·ªìm protocol)
            original_short_url: URL g·ªëc ƒë·ªÉ x√°c ƒë·ªãnh platform
        Returns:
            Web URL h·ª£p l·ªá ho·∫∑c None
        """
        try:
            import re
            from urllib.parse import parse_qs, urlparse
            
            logger.info(f"üîÑ Converting deep link: {deep_link_content}")
            
            # T√¨m product ID trong deep link content
            product_id = None
            
            # Pattern cho 1688 offerId
            if 'offerId=' in deep_link_content:
                match = re.search(r'offerId=(\d+)', deep_link_content)
                if match:
                    product_id = match.group(1)
                    logger.info(f"Found 1688 offerId: {product_id}")
            
            # Pattern cho taobao/tmall itemId
            elif 'id=' in deep_link_content:
                match = re.search(r'id=(\d+)', deep_link_content)
                if match:
                    product_id = match.group(1)
                    logger.info(f"Found itemId: {product_id}")
            
            # Pattern cho path-based IDs
            elif '/offer/' in deep_link_content:
                match = re.search(r'/offer/(\d+)', deep_link_content)
                if match:
                    product_id = match.group(1)
                    logger.info(f"Found 1688 path offerId: {product_id}")
            
            elif '/item/' in deep_link_content:
                match = re.search(r'/item/(\d+)', deep_link_content)
                if match:
                    product_id = match.group(1)
                    logger.info(f"Found path itemId: {product_id}")
            
            # T√¨m ID b·∫±ng regex chung
            if not product_id:
                id_match = re.search(r'(\d{9,13})', deep_link_content)
                if id_match:
                    product_id = id_match.group(1)
                    logger.info(f"Found generic ID: {product_id}")
            
            if not product_id:
                logger.warning("No product ID found in deep link")
                return None
            
            # X√°c ƒë·ªãnh platform v√† t·∫°o web URL
            if '1688' in deep_link_content or 'qr.1688.com' in original_short_url:
                web_url = f"https://detail.1688.com/offer/{product_id}.html"
                logger.info(f"‚úÖ Converted 1688 deep link: {web_url}")
                return web_url
            
            elif 'tmall' in deep_link_content or 'tmall' in original_short_url.lower():
                web_url = f"https://detail.tmall.com/item.htm?id={product_id}"
                logger.info(f"‚úÖ Converted Tmall deep link: {web_url}")
                return web_url
            
            elif 'taobao' in deep_link_content or 'taobao' in original_short_url.lower():
                web_url = f"https://item.taobao.com/item.htm?id={product_id}"
                logger.info(f"‚úÖ Converted Taobao deep link: {web_url}")
                return web_url
            
            else:
                # Default to 1688 n·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c
                web_url = f"https://detail.1688.com/offer/{product_id}.html"
                logger.info(f"‚úÖ Default 1688 conversion: {web_url}")
                return web_url
                
        except Exception as e:
            logger.error(f"‚ùå Deep link conversion failed: {e}")
            return None
    
    def _is_mobile_url(self, url: str) -> bool:
        """
        Ki·ªÉm tra xem URL c√≥ ph·∫£i l√† mobile URL kh√¥ng
        Args:
            url: URL c·∫ßn ki·ªÉm tra
        Returns:
            True n·∫øu l√† mobile URL
        """
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            netloc = parsed.netloc.lower()
            
            # Mobile domains
            mobile_domains = [
                'm.taobao.com',
                'h5.m.taobao.com', 
                'main.m.taobao.com',
                'm.tmall.com',
                'h5.tmall.com',
                'm.1688.com',
                'h5.1688.com'
            ]
            
            return any(domain in netloc for domain in mobile_domains)
            
        except Exception as e:
            logger.error(f"‚ùå Error checking mobile URL: {e}")
            return False
    
    def _convert_mobile_to_desktop_url(self, mobile_url: str) -> Optional[str]:
        """
        Convert mobile URL th√†nh desktop URL
        Args:
            mobile_url: Mobile URL c·∫ßn convert
        Returns:
            Desktop URL ho·∫∑c None
        """
        try:
            from urllib.parse import urlparse
            import re
            
            logger.info(f"üîÑ Converting mobile URL to desktop: {mobile_url}")
            
            # Extract product ID t·ª´ mobile URL
            product_id = self.extract_product_id(mobile_url)
            
            if not product_id:
                logger.warning("No product ID found in mobile URL")
                return None
            
            # X√°c ƒë·ªãnh platform v√† t·∫°o desktop URL
            parsed = urlparse(mobile_url)
            netloc = parsed.netloc.lower()
            
            if 'taobao.com' in netloc:
                desktop_url = f"https://item.taobao.com/item.htm?id={product_id}"
                logger.info(f"‚úÖ Converted Taobao mobile to desktop: {desktop_url}")
                return desktop_url
            
            elif 'tmall.com' in netloc:
                desktop_url = f"https://detail.tmall.com/item.htm?id={product_id}"
                logger.info(f"‚úÖ Converted Tmall mobile to desktop: {desktop_url}")
                return desktop_url
            
            elif '1688.com' in netloc:
                desktop_url = f"https://detail.1688.com/offer/{product_id}.html"
                logger.info(f"‚úÖ Converted 1688 mobile to desktop: {desktop_url}")
                return desktop_url
            
            else:
                logger.warning(f"Unknown platform for mobile URL: {netloc}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Mobile to desktop conversion failed: {e}")
            return None
    
    

    def extract_product_id(self, url: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t product ID t·ª´ URL"""
        try:
            parsed = urlparse(url)
            
            # Th·ª≠ l·∫•y t·ª´ query parameter 'id'
            from urllib.parse import parse_qs
            query_params = parse_qs(parsed.query)
            if 'id' in query_params:
                return query_params['id'][0]
            
            # Th·ª≠ t√¨m pattern s·ªë trong URL
            import re
            match = re.search(r'(?:id=|/item/)(\d+)', url)
            if match:
                return match.group(1)
                
            return None
            
        except Exception as e:
            logger.error(f"L·ªói khi extract product ID t·ª´ {url}: {e}")
            return None

# T·∫°o instance global ƒë·ªÉ s·ª≠ d·ª•ng
url_resolver = URLResolver()

def resolve_product_url(url: str) -> Dict[str, Any]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ resolve URL s·∫£n ph·∫©m
    H·ªó tr·ª£ c·∫£ URL thu·∫ßn t√∫y v√† text ch·ª©a URL
    
    Args:
        url: URL c·∫ßn resolve ho·∫∑c text ch·ª©a URL
        
    Returns:
        Dict ch·ª©a th√¥ng tin resolve result
    """
    return url_resolver.get_final_url(url)

def extract_urls_from_text(text: str) -> List[str]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ extract URLs t·ª´ text
    
    Args:
        text: Text ch·ª©a URL
        
    Returns:
        List c√°c URL ƒë∆∞·ª£c extract
    """
    return url_resolver.extract_urls_from_text(text)

def extract_best_url_from_text(text: str) -> Optional[str]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ extract URL t·ªët nh·∫•t t·ª´ text
    
    Args:
        text: Text ch·ª©a URL
        
    Returns:
        URL t·ªët nh·∫•t ho·∫∑c None
    """
    return url_resolver.extract_best_url_from_text(text)

def extract_product_id(url: str) -> Optional[str]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ extract product ID t·ª´ URL
    
    Args:
        url: URL ch·ª©a product ID
        
    Returns:
        Product ID string ho·∫∑c None
    """
    return url_resolver.extract_product_id(url)

# Test function
if __name__ == "__main__":
    # Test v·ªõi c√°c URL m·∫´u
    test_urls = [
        "https://e.tb.cn/h.SVYesMz1CWCGef8?tk=gGCY4DMdCiV",
        "https://detail.tmall.com/item.htm?id=777166626275",
        "https://item.taobao.com/item.htm?id=123456789",
        "https://google.com"  # Invalid URL
    ]
    
    # Test v·ªõi text ch·ª©a URL
    test_texts = [
        "„ÄêÊ∑òÂÆù„ÄëÂÅá‰∏ÄËµîÂõõ https://e.tb.cn/h.SU96zrxZvJOnr9h?tk=ORBN4yfCXn4 HU926 „ÄåÁ∫ØÊ¨≤È£éÈªëËâ≤ÊäπËÉ∏ËøûË°£Ë£ôÂ•≥2025ÁßãÊ≥ïÂºèËΩªÁÜüÈ£éËΩªÂ•¢Êî∂ËÖ∞ÊÄßÊÑüÂÆ¥‰ºöÁ§ºÊúçË£ô„ÄçÁÇπÂáªÈìæÊé•Áõ¥Êé•ÊâìÂºÄ ÊàñËÄÖ Ê∑òÂÆùÊêúÁ¥¢Áõ¥Êé•ÊâìÂºÄ",
        "„ÄêÊ∑òÂÆù„ÄëÂÅá‰∏ÄËµîÂõõ https://e.tb.cn/h.SfEU0GknEMtJgix?tk=pU7M4yfCR2L tG-#22>lD „ÄåÈ´òÁ∫ßÊÑüÈªëËâ≤ÈíàÁªáÊåÇËÑñËÉåÂøÉÂ•≥2025Â§èÊ≥ïÂºèÂ§çÂè§ÂêçÂ™õÈ£é‰øÆË∫´ÊòæÁò¶Áü≠Ê¨æ‰∏äË°£„ÄçÁÇπÂáªÈìæÊé•Áõ¥Êé•ÊâìÂºÄ ÊàñËÄÖ Ê∑òÂÆùÊêúÁ¥¢Áõ¥Êé•ÊâìÂºÄ",
        "Check out this product: https://detail.1688.com/offer/953742824238.html - great quality!"
    ]
    
    print("=== Test with pure URLs ===")
    for test_url in test_urls:
        result = resolve_product_url(test_url)
        print(f"\nTest URL: {test_url}")
        print(f"Result: {result}")
        
        if result['success'] and result['final_url']:
            product_id = extract_product_id(result['final_url'])
            print(f"Product ID: {product_id}")
    
    print("\n=== Test with text containing URLs ===")
    for test_text in test_texts:
        print(f"\nTest Text: {test_text[:100]}...")
        
        # Test extract URLs
        urls = extract_urls_from_text(test_text)
        print(f"Extracted URLs: {urls}")
        
        # Test extract best URL
        best_url = extract_best_url_from_text(test_text)
        print(f"Best URL: {best_url}")
        
        # Test resolve
        result = resolve_product_url(test_text)
        print(f"Resolve Result: {result}")
        
        if result['success'] and result['final_url']:
            product_id = extract_product_id(result['final_url'])
            print(f"Product ID: {product_id}")
