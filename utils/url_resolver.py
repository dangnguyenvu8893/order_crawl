"""
URL Resolver Utility
X·ª≠ l√Ω vi·ªác resolve URL redirect ƒë·ªÉ l·∫•y URL cu·ªëi c√πng t·ª´ short links
"""
import requests
import logging
import re
from typing import Optional, Dict, Any, List
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class URLResolver:
    def __init__(self):
        self.timeout = 10
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Referer': 'https://m.taobao.com/'
        }
        
        # C√°c domain c·∫ßn resolve
        self.short_domains = [
            'e.tb.cn',
            'tb.cn', 
            's.tb.cn',
            'm.tb.cn',
            's.click.taobao.com',
            'uland.taobao.com',
            'qr.1688.com'  # Th√™m h·ªó tr·ª£ 1688 QR links
        ]
        
        # C√°c domain ƒë√≠ch h·ª£p l·ªá
        self.target_domains = [
            'detail.tmall.com',
            'item.taobao.com',
            'detail.1688.com'
        ]
        
        # Regex patterns ƒë·ªÉ extract URL t·ª´ text
        # C·∫£i thi·ªán ƒë·ªÉ x·ª≠ l√Ω URL c√≥ k√Ω t·ª± th·ª´a ·ªü cu·ªëi
        self.url_extraction_patterns = [
            # Pattern ch√≠nh: URL v·ªõi domain v√† path, d·ª´ng ·ªü k√Ω t·ª± kh√¥ng h·ª£p l·ªá
            r'https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*)?',
            # Pattern cho short links (qr.1688.com, e.tb.cn, etc.)
            r'https?://(?:qr\.1688\.com|e\.tb\.cn|tb\.cn|s\.tb\.cn|m\.tb\.cn|s\.click\.taobao\.com|uland\.taobao\.com)/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*',
            # Pattern cho full product URLs
            r'https?://(?:detail\.tmall\.com|item\.taobao\.com|detail\.1688\.com)/[a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]*',
            # Fallback pattern cho c√°c URL kh√°c
            r'https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/[^\s]*',
        ]
        
        # Compiled patterns ƒë·ªÉ tƒÉng performance
        self.compiled_url_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.url_extraction_patterns]
    
    def is_short_url(self, url: str) -> bool:
        """Ki·ªÉm tra c√≥ ph·∫£i short URL kh√¥ng"""
        try:
            parsed = urlparse(url)
            return any(domain in parsed.netloc.lower() for domain in self.short_domains)
        except:
            return False
    
    def is_valid_target_url(self, url: str) -> bool:
        """Ki·ªÉm tra URL ƒë√≠ch c√≥ h·ª£p l·ªá kh√¥ng"""
        try:
            parsed = urlparse(url)
            return any(domain in parsed.netloc.lower() for domain in self.target_domains)
        except:
            return False
    
    def extract_urls_from_text(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ URL t·ª´ text c√≥ ch·ªØ tr∆∞·ªõc"""
        urls = []
        
        for pattern in self.compiled_url_patterns:
            matches = pattern.findall(text)
            urls.extend(matches)
        
        # Lo·∫°i b·ªè duplicate v√† clean up
        unique_urls = list(set(urls))
        cleaned_urls = [self._clean_url(url) for url in unique_urls]
        
        return [url for url in cleaned_urls if url]
    
    def _clean_url(self, url: str) -> str:
        """Clean up URL (lo·∫°i b·ªè k√Ω t·ª± th·ª´a ·ªü cu·ªëi)"""
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá ·ªü cu·ªëi URL
        url = url.rstrip('.,;!?Ôºâ„Äë„Äç ')
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i URL character ·ªü cu·ªëi
        import re
        url = re.sub(r'[^a-zA-Z0-9._~:/?#\[\]@!$&\'()*+,;=-]+$', '', url)
        
        # ƒê·∫£m b·∫£o URL k·∫øt th√∫c h·ª£p l·ªá
        if url.endswith('/'):
            url = url[:-1]
            
        return url
    
    def extract_best_url_from_text(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t URL t·ªët nh·∫•t t·ª´ text (∆∞u ti√™n URL c√≥ v·∫ª l√† s·∫£n ph·∫©m)"""
        urls = self.extract_urls_from_text(text)
        
        if not urls:
            return None
        
        # ∆Øu ti√™n URL c√≥ ch·ª©a t·ª´ kh√≥a s·∫£n ph·∫©m
        product_keywords = ['taobao', 'tmall', '1688', 'item', 'detail', 'offer']
        
        for url in urls:
            if any(keyword in url.lower() for keyword in product_keywords):
                return url
        
        # N·∫øu kh√¥ng c√≥ URL n√†o ch·ª©a t·ª´ kh√≥a, tr·∫£ v·ªÅ URL ƒë·∫ßu ti√™n
        return urls[0]
    
    def get_final_url(self, short_url: str) -> Dict[str, Any]:
        """
        Resolve short URL th√†nh final URL
        H·ªó tr·ª£ c·∫£ URL thu·∫ßn t√∫y v√† text c√≥ ch·ªØ tr∆∞·ªõc URL
        
        Args:
            short_url: URL c·∫ßn resolve ho·∫∑c text ch·ª©a URL
            
        Returns:
            Dict ch·ª©a th√¥ng tin resolve result
        """
        try:
            logger.info(f"üîç Resolving URL: {short_url}")
            
            # Ki·ªÉm tra xem input c√≥ ph·∫£i l√† text ch·ª©a URL kh√¥ng
            extracted_url = None
            if not short_url.startswith(('http://', 'https://')):
                # C√≥ th·ªÉ l√† text ch·ª©a URL, th·ª≠ extract
                extracted_url = self.extract_best_url_from_text(short_url)
                if extracted_url:
                    logger.info(f"üìù Extracted URL from text: {extracted_url}")
                    short_url = extracted_url
                else:
                    return {
                        'success': False,
                        'original_url': short_url,
                        'final_url': None,
                        'error': 'Kh√¥ng t√¨m th·∫•y URL h·ª£p l·ªá trong text',
                        'method': 'no_url_found'
                    }
            
            # N·∫øu ƒë√£ l√† URL h·ª£p l·ªá, return lu√¥n
            if self.is_valid_target_url(short_url):
                return {
                    'success': True,
                    'original_url': short_url,
                    'final_url': short_url,
                    'redirect_count': 0,
                    'method': 'no_redirect_needed',
                    'extracted_from_text': extracted_url is not None
                }
            
            # N·∫øu kh√¥ng ph·∫£i short URL, nh∆∞ng c≈©ng kh√¥ng ph·∫£i target URL
            if not self.is_short_url(short_url):
                return {
                    'success': False,
                    'original_url': short_url,
                    'final_url': None,
                    'error': 'URL kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£',
                    'method': 'unsupported_domain',
                    'extracted_from_text': extracted_url is not None
                }
            
            # Resolve short URL v·ªõi multiple strategies
            final_url, redirect_count = self._resolve_with_strategies(short_url)
            
            logger.info(f"‚úÖ Resolved: {short_url} ‚Üí {final_url} ({redirect_count} redirects)")
            
            # Ki·ªÉm tra final URL c√≥ h·ª£p l·ªá kh√¥ng
            if not self.is_valid_target_url(final_url):
                # N·∫øu kh√¥ng c√≥ redirect, c√≥ th·ªÉ link ƒë√£ h·∫øt h·∫°n
                if redirect_count == 0:
                    error_msg = 'Link r√∫t g·ªçn c√≥ th·ªÉ ƒë√£ h·∫øt h·∫°n ho·∫∑c kh√¥ng ho·∫°t ƒë·ªông. Vui l√≤ng th·ª≠ link ƒë·∫ßy ƒë·ªß t·ª´ trang s·∫£n ph·∫©m.'
                else:
                    error_msg = f'Link redirect ƒë·∫øn trang kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {final_url}'
                
                return {
                    'success': False,
                    'original_url': short_url,
                    'final_url': final_url,
                    'redirect_count': redirect_count,
                    'error': error_msg,
                    'method': 'invalid_final_url'
                }
            
            return {
                'success': True,
                'original_url': short_url,
                'final_url': final_url,
                'redirect_count': redirect_count,
                'method': 'redirect_resolved',
                'extracted_from_text': extracted_url is not None
            }
            
        except requests.Timeout:
            logger.error(f"‚ùå Timeout khi resolve URL: {short_url}")
            return {
                'success': False,
                'original_url': short_url,
                'final_url': None,
                'error': 'Timeout khi resolve URL',
                'method': 'timeout'
            }
            
        except requests.RequestException as e:
            logger.error(f"‚ùå L·ªói request khi resolve URL {short_url}: {e}")
            return {
                'success': False,
                'original_url': short_url,
                'final_url': None,
                'error': f'L·ªói request: {str(e)}',
                'method': 'request_error'
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi resolve URL {short_url}: {e}")
            return {
                'success': False,
                'original_url': short_url,
                'final_url': None,
                'error': f'L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}',
                'method': 'unknown_error'
            }
    
    def _resolve_with_strategies(self, short_url: str) -> tuple:
        """
        Resolve URL: Th·ª≠ HTTP redirect tr∆∞·ªõc, n·∫øu kh√¥ng c√≥ th√¨ parse content
        Returns: (final_url, redirect_count)
        """
        try:
            logger.info(f"üîç Resolving: {short_url}")
            
            # Strategy 1: Th·ª≠ HTTP redirect tr∆∞·ªõc
            response = requests.head(
                short_url, 
                headers=self.headers, 
                allow_redirects=True, 
                timeout=self.timeout
            )
            
            final_url = response.url
            redirect_count = len(response.history)
            
            # N·∫øu c√≥ redirect, return ngay
            if redirect_count > 0 or self.is_valid_target_url(final_url):
                logger.info(f"‚úÖ HTTP redirect: {short_url} ‚Üí {final_url} ({redirect_count} redirects)")
                return final_url, redirect_count
            
            # Strategy 2: N·∫øu kh√¥ng c√≥ HTTP redirect, parse content
            logger.info("No HTTP redirect, trying content parsing...")
            return self._parse_content_for_url(short_url)
            
        except requests.RequestException as e:
            logger.error(f"‚ùå Request failed: {e}")
            return short_url, 0
    
    def _parse_content_for_url(self, short_url: str) -> tuple:
        """
        Parse HTML content ƒë·ªÉ t√¨m URL ƒë√≠ch
        Returns: (final_url, redirect_count)
        """
        try:
            # GET request ƒë·ªÉ l·∫•y content
            response = requests.get(
                short_url, 
                headers=self.headers, 
                timeout=self.timeout
            )
            
            content = response.text
            logger.info(f"Got content: {len(content)} bytes")
            
            # T√¨m product URLs trong content
            product_patterns = [
                r'href=["\']([^"\']*detail\.tmall\.com[^"\']*)["\']',
                r'href=["\']([^"\']*item\.taobao\.com[^"\']*)["\']',
                r'href=["\']([^"\']*detail\.1688\.com[^"\']*)["\']',
                r'url=["\']([^"\']*detail\.tmall\.com[^"\']*)["\']',
                r'url=["\']([^"\']*item\.taobao\.com[^"\']*)["\']',
                r'url=["\']([^"\']*detail\.1688\.com[^"\']*)["\']',
                r'["\']([^"\']*detail\.tmall\.com[^"\']*)["\']',
                r'["\']([^"\']*item\.taobao\.com[^"\']*)["\']',
                r'["\']([^"\']*detail\.1688\.com[^"\']*)["\']'
            ]
            
            import re
            for pattern in product_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    # Filter valid URLs (c√≥ id= v√† ƒë·ªß d√†i)
                    valid_matches = [m for m in matches if 'id=' in m and len(m) > 20]
                    if valid_matches:
                        final_url = valid_matches[0]
                        logger.info(f"‚úÖ Content parsing: {short_url} ‚Üí {final_url}")
                        return final_url, 1  # Simulate 1 redirect
            
            # N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m product ID
            id_patterns = [
                r'itemId["\']?\s*:\s*["\']?(\d+)["\']?',
                r'item_id["\']?\s*:\s*["\']?(\d+)["\']?',
                r'id["\']?\s*:\s*["\']?(\d{9,13})["\']?',
                r'productId["\']?\s*:\s*["\']?(\d+)["\']?',
                r'offerId=(\d+)',  # Th√™m pattern cho 1688 offerId
                r'offer\.id=(\d+)',  # Th√™m pattern kh√°c cho 1688
                r'offer/(\d+)\.html'  # Th√™m pattern t·ª´ URL path
            ]
            
            for pattern in id_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    product_id = matches[0]
                    
                    # X√°c ƒë·ªãnh domain d·ª±a tr√™n short_url
                    if 'qr.1688.com' in short_url or '1688.com' in short_url:
                        # Construct 1688 URL
                        final_url = f"https://detail.1688.com/offer/{product_id}.html"
                        logger.info(f"‚úÖ 1688 ID extraction: {short_url} ‚Üí {final_url}")
                    else:
                        # Construct URL (default to Taobao)
                        final_url = f"https://item.taobao.com/item.htm?id={product_id}"
                        logger.info(f"‚úÖ ID extraction: {short_url} ‚Üí {final_url}")
                    
                    return final_url, 1
            
            logger.warning("No product URL or ID found in content")
            return short_url, 0
            
        except Exception as e:
            logger.error(f"‚ùå Content parsing failed: {e}")
            return short_url, 0
    
    

    def extract_product_id(self, url: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t product ID t·ª´ URL"""
        try:
            parsed = urlparse(url)
            
            # Th·ª≠ l·∫•y t·ª´ query parameter 'id'
            from urllib.parse import parse_qs
            query_params = parse_qs(parsed.query)
            if 'id' in query_params:
                return query_params['id'][0]
            
            # Th·ª≠ t√¨m pattern s·ªë trong URL
            import re
            match = re.search(r'(?:id=|/item/)(\d+)', url)
            if match:
                return match.group(1)
                
            return None
            
        except Exception as e:
            logger.error(f"L·ªói khi extract product ID t·ª´ {url}: {e}")
            return None

# T·∫°o instance global ƒë·ªÉ s·ª≠ d·ª•ng
url_resolver = URLResolver()

def resolve_product_url(url: str) -> Dict[str, Any]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ resolve URL s·∫£n ph·∫©m
    H·ªó tr·ª£ c·∫£ URL thu·∫ßn t√∫y v√† text ch·ª©a URL
    
    Args:
        url: URL c·∫ßn resolve ho·∫∑c text ch·ª©a URL
        
    Returns:
        Dict ch·ª©a th√¥ng tin resolve result
    """
    return url_resolver.get_final_url(url)

def extract_urls_from_text(text: str) -> List[str]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ extract URLs t·ª´ text
    
    Args:
        text: Text ch·ª©a URL
        
    Returns:
        List c√°c URL ƒë∆∞·ª£c extract
    """
    return url_resolver.extract_urls_from_text(text)

def extract_best_url_from_text(text: str) -> Optional[str]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ extract URL t·ªët nh·∫•t t·ª´ text
    
    Args:
        text: Text ch·ª©a URL
        
    Returns:
        URL t·ªët nh·∫•t ho·∫∑c None
    """
    return url_resolver.extract_best_url_from_text(text)

def extract_product_id(url: str) -> Optional[str]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ extract product ID t·ª´ URL
    
    Args:
        url: URL ch·ª©a product ID
        
    Returns:
        Product ID string ho·∫∑c None
    """
    return url_resolver.extract_product_id(url)

# Test function
if __name__ == "__main__":
    # Test v·ªõi c√°c URL m·∫´u
    test_urls = [
        "https://e.tb.cn/h.SVYesMz1CWCGef8?tk=gGCY4DMdCiV",
        "https://detail.tmall.com/item.htm?id=777166626275",
        "https://item.taobao.com/item.htm?id=123456789",
        "https://google.com"  # Invalid URL
    ]
    
    # Test v·ªõi text ch·ª©a URL
    test_texts = [
        "„ÄêÊ∑òÂÆù„ÄëÂÅá‰∏ÄËµîÂõõ https://e.tb.cn/h.SU96zrxZvJOnr9h?tk=ORBN4yfCXn4 HU926 „ÄåÁ∫ØÊ¨≤È£éÈªëËâ≤ÊäπËÉ∏ËøûË°£Ë£ôÂ•≥2025ÁßãÊ≥ïÂºèËΩªÁÜüÈ£éËΩªÂ•¢Êî∂ËÖ∞ÊÄßÊÑüÂÆ¥‰ºöÁ§ºÊúçË£ô„ÄçÁÇπÂáªÈìæÊé•Áõ¥Êé•ÊâìÂºÄ ÊàñËÄÖ Ê∑òÂÆùÊêúÁ¥¢Áõ¥Êé•ÊâìÂºÄ",
        "„ÄêÊ∑òÂÆù„ÄëÂÅá‰∏ÄËµîÂõõ https://e.tb.cn/h.SfEU0GknEMtJgix?tk=pU7M4yfCR2L tG-#22>lD „ÄåÈ´òÁ∫ßÊÑüÈªëËâ≤ÈíàÁªáÊåÇËÑñËÉåÂøÉÂ•≥2025Â§èÊ≥ïÂºèÂ§çÂè§ÂêçÂ™õÈ£é‰øÆË∫´ÊòæÁò¶Áü≠Ê¨æ‰∏äË°£„ÄçÁÇπÂáªÈìæÊé•Áõ¥Êé•ÊâìÂºÄ ÊàñËÄÖ Ê∑òÂÆùÊêúÁ¥¢Áõ¥Êé•ÊâìÂºÄ",
        "Check out this product: https://detail.1688.com/offer/953742824238.html - great quality!"
    ]
    
    print("=== Test with pure URLs ===")
    for test_url in test_urls:
        result = resolve_product_url(test_url)
        print(f"\nTest URL: {test_url}")
        print(f"Result: {result}")
        
        if result['success'] and result['final_url']:
            product_id = extract_product_id(result['final_url'])
            print(f"Product ID: {product_id}")
    
    print("\n=== Test with text containing URLs ===")
    for test_text in test_texts:
        print(f"\nTest Text: {test_text[:100]}...")
        
        # Test extract URLs
        urls = extract_urls_from_text(test_text)
        print(f"Extracted URLs: {urls}")
        
        # Test extract best URL
        best_url = extract_best_url_from_text(test_text)
        print(f"Best URL: {best_url}")
        
        # Test resolve
        result = resolve_product_url(test_text)
        print(f"Resolve Result: {result}")
        
        if result['success'] and result['final_url']:
            product_id = extract_product_id(result['final_url'])
            print(f"Product ID: {product_id}")
